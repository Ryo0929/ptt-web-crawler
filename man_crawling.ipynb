{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from anytree import Node, RenderTree\n",
    "import time\n",
    "from PttWebCrawler.crawler import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_by_url(board,url):\n",
    "    base_url=\"https://www.ptt.cc/man/\"+board+\"/\"+url\n",
    "    r = requests.get(base_url)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        links=soup.find_all('a')\n",
    "        urls=[x.get('href') for x in links]\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_ids(root_url,board):\n",
    "    #Here we define index url as \"link\" eg:https://www.ptt.cc/man/marvel/DFB6/D6BD/index.html\n",
    "    #article url as \"article\" eg:https://www.ptt.cc/man/marvel/DFB6/D6BD/M.1477757823.A.549.html\n",
    "    layer1=[root_url]\n",
    "    layer_list=[layer1]\n",
    "    i=0\n",
    "    while True:\n",
    "        layer_list+=[[]]\n",
    "        print(\"retreving layer\",i+1)\n",
    "        current_layer=layer_list[i]\n",
    "        next_layer=layer_list[i+1]\n",
    "\n",
    "        current_links=[x for x in current_layer if x.endswith(\"/index.html\")]\n",
    "        for current_link_url in current_links:\n",
    "            next_urls=[]\n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    next_urls=get_urls_by_url(board,current_link_url)\n",
    "                except TypeError:\n",
    "                    time.sleep(1)\n",
    "                    print(\"error occur in link:\",current_link_url, \" ,try again\")\n",
    "                    continue\n",
    "                if next_urls==None:\n",
    "                    time.sleep(1)\n",
    "                    print(\"error occur in link:\",current_link_url, \" ,try again\")\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            # leave urls end with \"M.xxxxx\" and \"xxx/xxx/index.html\"\n",
    "            next_urls=[x for x in next_urls if x.startswith(\"/man/\"+board+\"/\") and not x.endswith(current_link_url) and x!=\"/man/\"+board+\"/index.html\"]\n",
    "            next_layer+=[x.replace(\"/man/\"+board+\"/\",\"\") for x in next_urls]\n",
    "        if i>0:\n",
    "            pre_layer=layer_list[i-1]\n",
    "            layer_list[i+1]=[x for x in next_layer if x not in pre_layer]\n",
    "        have_next_layer=any([x for x in layer_list[i+1] if x.endswith(\"/index.html\")])\n",
    "        i+=1\n",
    "        print(\"total links: \",len(layer_list[i]))\n",
    "        if have_next_layer is False:\n",
    "            print(\"layer\",str(i),\" is the last layer\")\n",
    "            break\n",
    "    #get all article (links end with M.xxxx)\n",
    "    all_article=[]\n",
    "    for layer in layer_list:\n",
    "        if len(layer) >0:\n",
    "            all_article+=[x for x in layer if not x.endswith(\"index.html\")]\n",
    "    all_index_page=[]        \n",
    "    for layer in layer_list:\n",
    "        if len(layer) >0:\n",
    "            all_index_page+=[x for x in layer if x.endswith(\"index.html\")]\n",
    "    #remove \".html\"\n",
    "    all_article=[x[:-5] for x in all_article]\n",
    "    write_list_to_txt(board+\"-\"+root_url,all_article)\n",
    "    print(\"--------\")\n",
    "    print(\"Total articles:\",len(all_article))\n",
    "    print(\"Total index page:\",len(all_index_page))\n",
    "    return all_article\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_txt(file_name,article_list):\n",
    "    file_name=file_name.replace(\"/\",\"-\")\n",
    "    with open(file_name+\".txt\", 'w') as f:\n",
    "        for item in article_list:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_data_by_ids(board,id_list):\n",
    "    c = PttWebCrawler(as_lib=True)\n",
    "    count=0\n",
    "    for target in id_list:\n",
    "        try:\n",
    "            c.parse_article(target, board)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print(\"progress:\", count, \"/\", len(article_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type in the root url eg: \"DDF/index.html\"\n",
    "root_url=\"DDF/index.html\"\n",
    "# type in the board name eg: \"marvel\"\n",
    "board=\"marvel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retreving layer 1\n",
      "total links:  2\n",
      "retreving layer 2\n",
      "total links:  3\n",
      "retreving layer 3\n",
      "total links:  8\n",
      "layer 3  is the last layer\n",
      "--------\n",
      "Total articles: 8\n",
      "Total index page: 6\n"
     ]
    }
   ],
   "source": [
    "article_id_list=get_article_ids(root_url,board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: DDF-DD90-DDAA-M.1445699949.A.994\n",
      "Processing article: DDF-DD90-DDAA-M.1445859799.A.E46\n",
      "Processing article: DDF-DD90-DDAA-M.1445859806.A.3E6\n",
      "Processing article: DDF-DDB4-DDC6-M.1448103380.A.624\n",
      "Processing article: DDF-DDB4-DDC6-M.1448108011.A.3B7\n",
      "Processing article: DDF-DDB4-DDC6-M.1448108020.A.A5C\n",
      "Processing article: DDF-DDB4-DDC6-M.1448157088.A.6B8\n",
      "Processing article: DDF-DDB4-DDC6-M.1448157097.A.B16\n"
     ]
    }
   ],
   "source": [
    "crawling_data_by_ids(board,article_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
